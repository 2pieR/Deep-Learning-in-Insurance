{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "current_dir = os.getcwd();\n",
    "custom_bucket_boundary=[0,1,1000,250000] \n",
    "vocab_size=10000\n",
    "max_len=200\n",
    "max_words=10000\n",
    "max_features=10000\n",
    "embed_size=300\n",
    "sample=150 #sample reocrds\n",
    "train_size=1000#sample train\n",
    "dropout_rate=0.2\n",
    "size_embedding=15000\n",
    "epochs=50\n",
    "num_gpu=3\n",
    "#tensorboard_dir=\"./Payout\" #tensor board sub directory\n",
    "lbl = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir model#model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.getcwd() + '/model/payout_conv.hdf5' # save best models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir tensorB #tensorboard directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir=os.getcwd() + \"/tensorB/\" #tensor board graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow check\n",
    "tf.Session()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3]) \n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(sess.run(c))\n",
    "\n",
    "# Runs the op.\n",
    "# Log information\n",
    "options = tf.RunOptions(output_partition_graphs=True)\n",
    "metadata = tf.RunMetadata()\n",
    "c_val = sess.run(c, options=options, run_metadata=metadata)\n",
    "\n",
    "print(metadata.partition_graphs)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()\n",
    "df = pd.read_csv('paid.csv',low_memory =False)\n",
    "print(\"Shape of dataset {}\".format(df.shape))\n",
    "df.set_index('claim_claimnumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID column combination of other column\n",
    "ID = df['claimnumber'].astype(str)+ ',' + df['exposure_order'].astype(str) \n",
    "# replacing blanks with 0 in target variable\n",
    "df.total_paid.fillna(0,inplace=True) \n",
    "#dropping columns which has more than 20% blanks\n",
    "df=df.dropna(thresh=0.6*len(df),axis=1)\n",
    "#more columns to drop \n",
    "cols_to_drop=['claimnumber','exposure_order']\n",
    "#drop above columns\n",
    "df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "print('shape of dataframe {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.total_paid\n",
    "target.fillna(0,inplace=True)\n",
    "#TARGET BINNING\n",
    "bins = custom_bucket_boundary\n",
    "labels = ['No_pay','Below_1000','More_than_1K'] #labels\n",
    "#labels = ['1-500','500-1000','1000-2500','2500 greater']\n",
    "#CREATE BUCKETS\n",
    "df['payout_bucket'] = pd.cut(df['total_paid'], bins=bins, labels=labels)\n",
    "#REPLACING BLANKS WITH \n",
    "df['payout_bucket'].fillna('No_pay',inplace=True)\n",
    "print(\"Payout Bucket Distribution \\n{}\".format(df.payout_bucket.value_counts()))\n",
    "\n",
    "print(\"Label encoding buckets\")\n",
    "lbl.fit(list(df['payout_bucket'].values))\n",
    "df['payout_bucket_trans'] = lbl.transform(list(df['payout_bucket'].values))\n",
    "payout = df['payout_bucket_trans'].values\n",
    "print(\"Buckets after label encoding \\n{}\".format(df.payout_bucket_trans.value_counts()))\n",
    "\n",
    "#dropping target varaible from dataset\n",
    "df.drop(['total_paid','payout_bucket','payout_bucket_trans'],axis=1,inplace=True)\n",
    "print(\"target variable drop from training data, shape of training data {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify text & categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = 'claim_description'\n",
    "real_features = ['word_count','char_count','unique_word_count','word_density']\n",
    "cat_features = [col for col in df.columns if col not in [text_features] + real_features]\n",
    "#cat_features_hash = [col+\"_hash\" for col in cat_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean  categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean categorical data\n",
    "for col in cat_features:\n",
    "    if df[col].dtypes=='O':\n",
    "        df[col].fillna(\"unk\",inplace=True)  #replace blank categories as unknown\n",
    "        df[col]=df[col].apply(lambda x : clean_categorical_data(x))\n",
    "print(\"Categorical columns cleaning done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - Factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features:    \n",
    "    df[col] = pd.factorize(df[col])[0]\n",
    "trn_cat = df[cat_features]\n",
    "print(\"Categroical features are encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dummy varaibles for categorical features\n",
    "# df_cat = pd.get_dummies(df, columns=cat_features,\n",
    "#     sparse=True)\n",
    "#df_cat.drop(['claim_description'],axis=1,inplace=True)\n",
    "#trn_cat = df_cat.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim Description clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "df['claim_description'] =df.claim_description.apply(lambda x : clean(x))\n",
    "print(\"Time taken to clean {} mins\".format((time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count']= df.claim_description.apply(lambda x : len(str(x).split()))\n",
    "df['unique_word_count'] = df.claim_description.apply(lambda x : len(set(str(x).split())))\n",
    "df['char_count'] = df.claim_description.apply(len)\n",
    "df['word_density'] = df['char_count']/(df['word_count']+1)\n",
    "#cat_features = [col for col in df.columns if col not in text_features]\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing numeric columns\n",
    "mx = MinMaxScaler()\n",
    "df_real = mx.fit_transform(df[real_features])\n",
    "print(\"Shape of real features {}\".format(df_real.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknizer = Tokenizer(num_words=max_words)\n",
    "def tokenize(text):    \n",
    "    tknizer.fit_on_texts(text) #only 100 records are consider\n",
    "    trn_text = tknizer.texts_to_sequences(text)\n",
    "    train_text = pad_sequences(trn_text,maxlen=113)\n",
    "    return train_text\n",
    "train_text = tokenize(df['claim_description']) #text feature\n",
    "print(\"Text data shape {}\".format(train_text.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size =300\n",
    "max_features = 15000\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE= \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeff(word,*arr): return word, np.asarray(arr,dtype='float32')\n",
    "embedding_index = dict(get_coeff(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tknizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words,embed_size)) #np.random.normal(emb_mean,emb_std,(nb_words,embed_size))\n",
    "for word,i in word_index.items():\n",
    "    if  i >= max_features : continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None : embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# weights = class_weight.compute_class_weight('balanced',np.unique(payout),payout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable one hot \n",
    "target= tf.keras.utils.to_categorical(payout,num_classes=None)\n",
    "print(\"Output shape {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():    \n",
    "        # categorical channel\n",
    "        with tf.name_scope(\"Input_Category\"):      \n",
    "            inputs1 = Input(shape=(trn_cat.shape[1],))\n",
    "        with tf.name_scope(\"Dense_cat\"): \n",
    "            dense_cat_1 = Dense(256, activation='relu')(inputs1)\n",
    "            dense_cat_2 = Dense(128, activation='relu')(dense_cat_1)\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            drop1 = Dropout(0.2)(dense_cat_2)\n",
    "        with tf.name_scope('Flat_1'):\n",
    "            flat1 = Dense(32, activation='relu')(drop1)\n",
    "            \n",
    "        #numeric channel\n",
    "        with tf.name_scope(\"Numeric_channel\"):\n",
    "            inputs2 = Input(shape=(df_real.shape[1],))\n",
    "        with tf.name_scope(\"Dense_real\"):\n",
    "            dense_real_1=Dense(256,activation='relu')(inputs2)\n",
    "            dense_real_2 = Dense(128,activation='relu')(dense_real_1)\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            drop2 = Dropout(0.2)(dense_real_2)\n",
    "        with tf.name_scope('Flat_2'):\n",
    "            flat2=Dense(64,activation='relu')(drop2)\n",
    "\n",
    "        # text chanel\n",
    "        with tf.name_scope(\"Input_Text\"):\n",
    "            inputs3 = Input(shape=(train_text.shape[1],))\n",
    "            embedding3 = Embedding(size_embedding, 300,weights=[embedding_matrix],trainable = False)(inputs3)\n",
    "            x = SpatialDropout1D(0.2)(embedding3)\n",
    "        with tf.name_scope(\"LSTM\"):\n",
    "            x1 = Bidirectional(GRU(80,return_sequences=True,dropout=0.2,recurrent_dropout=0.25))(x)\n",
    "        with tf.name_scope(\"Convolution\"):\n",
    "            conv3 = Conv1D(filters=128, kernel_size=8,padding='valid',kernel_initializer='glorot_uniform')(x1)\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            drop3 = Dropout(0.2)(conv3)\n",
    "        with tf.name_scope('Average_pool'):\n",
    "            avg_pool = GlobalAveragePooling1D()(drop3)\n",
    "        with tf.name_scope('MaxPool'):\n",
    "            max_pool = GlobalMaxPooling1D()(drop3)\n",
    "        with tf.name_scope(\"Concat\"):\n",
    "            x = concatenate([avg_pool,max_pool])\n",
    " \n",
    "        # merge\n",
    "        with tf.name_scope('Merge_Channels'):\n",
    "            merged = concatenate([flat1,flat2,x])\n",
    "        with tf.name_scope(\"Dense\"):\n",
    "            dense1 = Dense(200, activation='relu')(merged)\n",
    "            dense2 = Dense(100, activation='relu')(dense1)\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            outputs = Dense(3, activation='sigmoid')(merged)\n",
    "        model1 = Model(inputs=[inputs1,inputs2,inputs3], outputs=outputs)\n",
    "        return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD,Adamax,Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CallBacks\n",
    "#check_point = ModelCheckpoint(model_save_dir, monitor = \"val_loss\", verbose = 1,\n",
    "#                              save_best_only = True, mode = \"min\")\n",
    "#early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "tbCallBack = TensorBoard(log_dir=tensorboard_dir,\n",
    "                         histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    with tf.name_scope('Compile'):\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=1e-5),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Single gpu/cpu train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in ['/device:GPU:0','/device:GPU:1','/device:GPU:2']:\n",
    "with tf.device('/device:GPU:1'):\n",
    "    model.fit([trn_cat[:60000],df_real[:60000], train_text[:60000]], target[:60000], batch_size=128, epochs=3, validation_split=0.15,callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in ['/device:GPU:0','/device:GPU:1','/device:GPU:2']:\n",
    "with tf.device('/device:GPU:1'):\n",
    "    model.fit([trn_cat, train_text], target, batch_size=128, epochs=10, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-gpu train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For multi gpus \n",
    "parallel_model = multi_gpu_model(model,num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    parallel_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=1e-5),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with real channel\n",
    "#one iteration on multiple gpu on complete dataset\n",
    "parallel_model.fit([trn_cat,df_real,train_text], target, batch_size=128, epochs=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    parallel_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=1e-6),metrics=['accuracy'])\n",
    "#lr = -6\n",
    "#one iteration on multiple gpu on complete dataset\n",
    "parallel_model.fit([trn_cat,df_real,train_text], target, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "with tf.device('/device:cpu:0'):\n",
    "    with tf.name_scope('Compile'):\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=Nadam(lr=1e-5),metrics=['accuracy'])\n",
    "parallel_model = multi_gpu_model(model,3)\n",
    "with tf.device('/cpu:0'):\n",
    "    parallel_model.compile(loss='categorical_crossentropy',optimizer=Nadam(lr=1e-5),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one iteration on multiple gpu on complete dataset\n",
    "parallel_model.fit([trn_cat,train_text], target, batch_size=128, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only after model is finalize, not before that, validation set is already there for predictions\n",
    "preds = model.predict([trn_cat[train_size:size],trn_text[train_size:size]],batch_size=batch_size)\n",
    "preds_classes = preds.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
