{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only one gpu visibie\n",
    "import os\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "current_dir = os.getcwd();\n",
    "custom_bucket_boundary=[0,1,1000,250000] \n",
    "vocab_size=10000\n",
    "max_len=200\n",
    "max_words=10000\n",
    "embed_size=300\n",
    "max_features=10000\n",
    "sample=150 #sample reocrds\n",
    "train_size=1000#sample train\n",
    "dropout_rate=0.2\n",
    "size_embedding=10000\n",
    "epochs=50\n",
    "num_gpu=3\n",
    "#tensorboard_dir=\"./Payout\" #tensor board sub directory\n",
    "lbl = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir model#model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.getcwd() + '/model/binary_model_classifier.hdf5' # save best models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir tensorB #tensorboard directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir=os.getcwd() + \"/tensorB/\" #tensor board graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow check\n",
    "tf.Session()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3]) \n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(sess.run(c))\n",
    "\n",
    "# Runs the op.\n",
    "# Log information\n",
    "options = tf.RunOptions(output_partition_graphs=True)\n",
    "metadata = tf.RunMetadata()\n",
    "c_val = sess.run(c, options=options, run_metadata=metadata)\n",
    "\n",
    "print(metadata.partition_graphs)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data file\n",
    "path=os.getcwd()\n",
    "df = pd.read_csv('paid.csv',low_memory =False)\n",
    "print(\"Shape of dataset {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnamed column\n",
    "df =df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "print(\"shape of dataframe after removing columns {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID column combination of other column\n",
    "ID = df['claimnumber'].astype(str)+ ',' + df['exp_claimorder'].astype(str) \n",
    "# replacing blanks with 0 in target variable\n",
    "df.total_paid.fillna(0,inplace=True) \n",
    "#dropping columns which has more than 20% blanks\n",
    "df=df.dropna(thresh=0.6*len(df),axis=1)\n",
    "#more columns to drop \n",
    "cols_to_drop=['claimnumber','exp_claimorder']\n",
    "#drop above columns\n",
    "df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "print('shape of dataframe {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.total_paid\n",
    "target.fillna(0,inplace=True)\n",
    "#TARGET BINNING\n",
    "bins = [0,1,200000]\n",
    "#labels = ['No_pay','Below_1000','More_than_1K'] #labels\n",
    "labels = ['No_pay','Yes_pay']\n",
    "#CREATE BUCKETS\n",
    "df['payout_bucket'] = pd.cut(df['total_paid'], bins=bins, labels=labels)\n",
    "#REPLACING BLANKS WITH \n",
    "df['payout_bucket'].fillna('No_pay',inplace=True)\n",
    "print(\"Payout Bucket Distribution \\n{}\".format(df.payout_bucket.value_counts()))\n",
    "\n",
    "print(\"Label encoding buckets\")\n",
    "lbl.fit(list(df['payout_bucket'].values))\n",
    "df['payout_bucket_trans'] = lbl.transform(list(df['payout_bucket'].values))\n",
    "payout = df['payout_bucket_trans'].values\n",
    "print(\"Buckets after label encoding \\n{}\".format(df.payout_bucket_trans.value_counts()))\n",
    "\n",
    "#dropping target varaible from dataset\n",
    "df.drop(['total_paid','payout_bucket','payout_bucket_trans'],axis=1,inplace=True)\n",
    "print(\"target variable drop from training data, shape of training data {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify text & categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = 'claim_description'\n",
    "cat_features = [col for col in df.columns if col not in text_features]\n",
    "#cat_features_hash = [col+\"_hash\" for col in cat_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean  categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean categorical data\n",
    "for col in cat_features:\n",
    "    if df[col].dtypes=='O':\n",
    "        df[col].fillna(\"unk\",inplace=True)  #replace blank categories as unknown\n",
    "        df[col]=df[col].apply(lambda x : clean_categorical_data(x))\n",
    "print(\"Categorical columns cleaning done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - Factorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Factorize\n",
    "for col in cat_features:    \n",
    "    df[col] = pd.factorize(df[col])[0]\n",
    "trn_cat = df[cat_features] #categorical features\n",
    "print(\"Categroical features are encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy varaibles for categorical features\n",
    "#df_cat = pd.get_dummies(df, columns=cat_features,\n",
    " #   sparse=True)\n",
    "#df_cat.drop(['claim_description'],axis=1,inplace=True)\n",
    "#trn_cat = df_cat.values #categorical features\n",
    "#print(\"Shape of categorical varaibles {}\".format(df_cat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim Description clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "df['claim_description'] =df.claim_description.apply(lambda x : clean(x))\n",
    "print(\"Time taken to clean {} mins\".format((time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknizer = Tokenizer(num_words=max_words)\n",
    "def tokenize(text):    \n",
    "    tknizer.fit_on_texts(text) #only 100 records are consider\n",
    "    trn_text = tknizer.texts_to_sequences(text)\n",
    "    train_text = pad_sequences(trn_text,maxlen=200)\n",
    "    return train_text\n",
    "train_text = tokenize(df['claim_description']) #text feature\n",
    "print(\"Text data shape {}\".format(train_text.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMbed file path\n",
    "EMBEDDING_FILE= \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read embed file\n",
    "def get_coeff(word,*arr): return word, np.asarray(arr,dtype='float32')\n",
    "embedding_index = dict(get_coeff(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create EMbedding Matrix\n",
    "word_index = tknizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words,embed_size)) #np.random.normal(emb_mean,emb_std,(nb_words,embed_size))\n",
    "for word,i in word_index.items():\n",
    "    if  i >= max_features : continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None : embedding_matrix[i] = embedding_vector\n",
    "print(\"Shape of EMbedding matrix {}\".format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.utils import class_weight\n",
    "#weights = class_weight.compute_class_weight('balanced',np.unique(payout),payout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable one hot \n",
    "target= tf.keras.utils.to_categorical(payout,num_classes=None)\n",
    "print(\"Output shape {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD,Adamax,Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new Model Arch\n",
    "def get_model():    \n",
    "         # categorical channel\n",
    "        with tf.name_scope(\"Input_Category\"):      \n",
    "            inputs1 = Input(shape=(trn_cat.shape[1],))\n",
    "        with tf.name_scope(\"Dense_cat\"): \n",
    "            dense_cat_1 = Dense(256, activation='relu')(inputs1)\n",
    "            dense_cat_2 = Dense(128, activation='relu')(dense_cat_1)\n",
    "        with tf.name_scope('Flat_1'):\n",
    "            flat1 = Dense(32, activation='relu')(dense_cat_2)\n",
    "\n",
    "        # text chanel\n",
    "        with tf.name_scope(\"Input_Text\"):\n",
    "            inputs3 = Input(shape=(train_text.shape[1],))\n",
    "            embedding3 = Embedding(size_embedding, 300,weights=[embedding_matrix],trainable = False)(inputs3)\n",
    "        with tf.name_scope(\"Convolution\"):\n",
    "            conv3 = Conv1D(filters=128, kernel_size=8,padding='valid',kernel_initializer='glorot_uniform')(embedding3)\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            drop3 = Dropout(0.3)(conv3)\n",
    "        with tf.name_scope('Average_pool'):\n",
    "            avg_pool = GlobalAveragePooling1D()(conv3)\n",
    "        with tf.name_scope('MaxPool'):\n",
    "            max_pool = GlobalMaxPooling1D()(conv3)\n",
    "        with tf.name_scope(\"Concat\"):\n",
    "            x = concatenate([avg_pool,max_pool])\n",
    "\n",
    "        # merge\n",
    "        with tf.name_scope('Merge_Channels'):\n",
    "            merged = concatenate([flat1,x])\n",
    "        with tf.name_scope(\"Dense\"):\n",
    "            dense1 = Dense(200, activation='relu')(merged)\n",
    "            dense2 = Dense(100, activation='relu')(dense1)\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            outputs = Dense(2, activation='sigmoid')(merged)\n",
    "        model1 = Model(inputs=[inputs1,inputs3], outputs=outputs)\n",
    "        return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single CPU/GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    model.compile(Adam(lr=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([trn_cat,train_text],target,batch_size=128,epochs=3,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi GPU Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model = multi_gpu_model(model,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    parallel_model.compile(Adam(lr=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model.fit([trn_cat,train_text],target,batch_size=128,epochs=3,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model,parallel_model,df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model,parallel_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Architecture - RNN & CNN combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():    \n",
    "        # categorical channel\n",
    "        with tf.name_scope(\"Input_Category\"):      \n",
    "            inputs1 = Input(shape=(trn_cat.shape[1],))\n",
    "        with tf.name_scope(\"Dense_cat\"): \n",
    "            dense_cat_1 = Dense(256, activation='relu')(inputs1)\n",
    "            dense_cat_2 = Dense(128, activation='relu')(dense_cat_1)\n",
    "        with tf.name_scope('Flat_1'):\n",
    "            flat1 = Dense(32, activation='relu')(dense_cat_2)\n",
    "\n",
    "        # text chanel\n",
    "        with tf.name_scope(\"Input_Text\"):\n",
    "            inputs3 = Input(shape=(train_text.shape[1],))\n",
    "            embedding3 = Embedding(size_embedding, 300,weights=[embedding_matrix],trainable = False)(inputs3)\n",
    "            x = SpatialDropout1D(0.2)(embedding3)\n",
    "        with tf.name_scope(\"LSTM\"):\n",
    "            x1 = Bidirectional(GRU(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.25))(embedding3)\n",
    "        with tf.name_scope(\"Convolution\"):\n",
    "            conv3 = Conv1D(filters=128, kernel_size=8,padding='valid',kernel_initializer='glorot_uniform')(embedding3)\n",
    "        with tf.name_scope(\"Dropout\"):\n",
    "            drop3 = Dropout(0.3)(conv3)\n",
    "        with tf.name_scope('Average_pool'):\n",
    "            avg_pool = GlobalAveragePooling1D()(conv3)\n",
    "        with tf.name_scope('MaxPool'):\n",
    "            max_pool = GlobalMaxPooling1D()(conv3)\n",
    "        with tf.name_scope(\"Concat\"):\n",
    "            x = concatenate([avg_pool,max_pool])\n",
    "  \n",
    "        # merge\n",
    "        with tf.name_scope('Merge_Channels'):\n",
    "            merged = concatenate([flat1,x])\n",
    "        with tf.name_scope(\"Dense\"):\n",
    "            dense1 = Dense(200, activation='relu')(merged)\n",
    "            dense2 = Dense(100, activation='relu')(dense1)\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            outputs = Dense(2, activation='sigmoid')(merged)\n",
    "        model1 = Model(inputs=[inputs1,inputs3], outputs=outputs)\n",
    "        return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single CPU/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "with tf.device('/device:cpu:0'):\n",
    "    model.compile(Adam(lr=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "with tf.device('/device:gpu:3'):\n",
    "    model.fit([trn_cat,train_text],target,batch_size=128,epochs=3,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chainge learing rate of the model training\n",
    "with tf.device('/device:cpu:0'):\n",
    "    model.compile(Adam(lr=1e-3),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runing on gpu\n",
    "with tf.device('/device:gpu:3'):\n",
    "    model.fit([trn_cat,train_text],target,batch_size=128,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model,parallel_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
